apiVersion: v1
kind: ConfigMap
metadata:
  namespace: pingcap
  name: v1.0.3-config-template
data:
  pump-config: |-
    # pump Configuration.

    # addr(i.e. 'host:port') to listen on for client traffic
    addr = "127.0.0.1:8250"

    # addr(i.e. 'host:port') to advertise to the public
    advertise-addr = ""

    # a integer value to control expiry date of the binlog data, indicates for how long (in days) the binlog data would be stored.
    # (default value is 0, means binlog data would never be removed)
    gc = 7

    # path to the data directory of pump's data
    data-dir = "data.pump"

    # kafka where pump push binlog to
    <%- if eq .KafkaAddr "" %>
    # kafka-addrs = "kafka-0.kafka-svc.default.svc.cluster.local:9092"
    <%- else %>
    kafka-addrs = "<% .KafkaAddr %>"
    <%- end %>

    # zookeeper addrs, get kafka-addrs from zookeeper if uncomment this
    # zookeeper-addrs = "zk-0.zk-svc.default.svc.cluster.local:2181"

    # number of seconds between heartbeat ticks (in 2 seconds)
    heartbeat-interval = 2

    # a comma separated list of PD endpoints
    pd-urls = "http://127.0.0.1:2379"

    # unix socket addr to listen on for client traffic
    socket = "unix:///var/run/tidb/pump.sock"

  pd-config: |-
    # PD Configuration.

    name = "pd"
    data-dir = "default.pd"

    client-urls = "http://127.0.0.1:2379"
    # if not set, use ${client-urls}
    advertise-client-urls = ""

    peer-urls = "http://127.0.0.1:2380"
    # if not set, use ${peer-urls}
    advertise-peer-urls = ""

    initial-cluster = ""
    initial-cluster-state = ""

    lease = 3
    tso-save-interval = "3s"

    [log]
    level = "info"

    # log format, one of json, text, console
    #format = "text"

    # disable automatic timestamps in output
    #disable-timestamp = false

    # file logging
    [log.file]
    #filename = ""
    # max log file size in MB
    #max-size = 300
    # max log file keep days
    #max-days = 28
    # maximum number of old log files to retain
    #max-backups = 7
    # rotate log by day
    #log-rotate = true

    [metric]
    # prometheus client push interval, set "0s" to disable prometheus.
    interval = "15s"
    # prometheus pushgateway address, leaves it empty will disable prometheus.
    # address = "{{.MetricsAddr}}"

    [schedule]
    max-snapshot-count = 3
    max-store-down-time = "1h"
    leader-schedule-limit = 64
    region-schedule-limit = 16
    replica-schedule-limit = 24

    # customized schedulers, the format is as below
    # if empty, it will use balance-leader, balance-region, hot-region as default
    # [[schedule.schedulers]]
    # type = "evict-leader"
    # args = ["1"]


    [replication]
    # The number of replicas for each region.
    max-replicas = 3
    # The label keys specified the location of a store.
    # The placement priorities is implied by the order of label keys.
    # For example, ["zone", "rack"] means that we should place replicas to
    # different zones first, then to different racks if we don't have enough zones.
    location-labels = ["zone", "rack", "host"]

  tidb-config: |-
    # TiDB Configuration.

    # TiDB server host.
    host = "0.0.0.0"

    # TiDB server port.
    port = 4000

    # Registered store name, [memory, goleveldb, boltdb, tikv, mocktikv]
    store = "mocktikv"

    # TiDB storage path.
    path = "/tmp/tidb"

    # The socket file to use for connection.
    #socket = ""

    # Socket file to write binlog.
    #binlog-socket = ""

    # Run ddl worker on this tidb-server.
    run-ddl = true

    # Schema lease duration, very dangerous to change only if you know what you do.
    lease = "<% .TiDBLease %>s"

    # When create table, split a separated region for it.
    # split-table = false

    [log]
    # Log level: info, debug, warn, error, fatal.
    level = "info"

    # Log format, one of json, text, console.
    format = "text"

    # Disable automatic timestamps in output
    disable-timestamp = false

    # Queries with execution time greater than this value will be logged. (Milliseconds)
    slow-threshold = 300

    # Maximum query length recorded in log.
    query-log-max-len = 2048

    # File logging.
    [log.file]
    # Log file name.
    filename = ""

    # Max log file size in MB.
    #max-size = 300

    # Max log file keep days.
    #max-days = 28

    # Maximum number of old log files to retain.
    #max-backups = 7

    # Rotate log by day
    log-rotate = true

    [security]
    # This option causes the server to start without using the privilege system at all.
    skip-grant-table = <% .SkipGrantTable %>

    # Path of file that contains list of trusted SSL CAs.
    ssl-ca = ""

    # Path of file that contains X509 certificate in PEM format.
    ssl-cert = ""

    # Path of file that contains X509 key in PEM format.
    ssl-key = ""

    [status]
    # If enable status report HTTP service.
    report-status = true

    # TiDB status port.
    status-port = 10080

    # Prometheus pushgateway address, leaves it empty will disable prometheus push.
    # metrics-addr = "{{.MetricsAddr}}"

    # Prometheus client push interval in second, set \"0\" to disable prometheus push.
    metrics-interval = 15

    [performance]
    # Set keep alive option for tcp connection.
    tcp-keep-alive = true

    # The maximum number of retries when commit a transaction.
    retry-limit = 10

    # The number of goroutines that participate joining.
    join-concurrency = 5

    # Whether support cartesian product.
    cross-join = true

    # Stats lease duration, which inflences the time of analyze and stats load.
    stats-lease = "3s"

    # Run auto analyze worker on this tidb-server.
    run-auto-analyze = true

    [xprotocol]
    # Start TiDB x server.
    xserver = false

    # TiDB x protocol server host.
    xhost = "0.0.0.0"

    # TiDB x protocol server port.
    xport = 14000

    # The socket file to use for x protocol connection.
    xsocket = ""

    [plan-cache]
    plan-cache-enabled = false
    plan-cache-capacity = 2560
    plan-cache-shards = 256

  tikv-config: |-
    # TiKV config template
    #  Human-readable big numbers:
    #   File size(based on byte): KB, MB, GB, TB, PB
    #    e.g.: 1_048_576 = "1MB"
    #   Time(based on ms): ms, s, m, h
    #    e.g.: 78_000 = "1.3m"

    # log level: trace, debug, info, warn, error, off.
    # log-level = "info"
    # file to store log, write to stderr if it's empty.
    # log-file = ""

    [server]
    # set listening address.
    # addr = "127.0.0.1:20160"
    # set advertise listening address for client communication, if not set, use addr instead.
    # advertise-addr = ""
    # notify capacity, 40960 is suitable for about 7000 regions.
    # notify-capacity = 40960
    # maximum number of messages can be processed in one tick.
    # messages-per-tick = 4096

    # size of thread pool for grpc server.
    # grpc-concurrency = 4
    # The number of max concurrent streams/requests on a client connection.
    # grpc-concurrent-stream = 1024
    # The number of connections with each tikv server to send raft messages.
    # grpc-raft-conn-num = 10
    # Amount to read ahead on individual grpc streams.
    # grpc-stream-initial-window-size = "2MB"

    # size of thread pool for endpoint task, should less than total cpu cores.
    <%- if eq .EndpointConcurrency 0 %>
    # end-point-concurrency = 8
    <%- else %>
    end-point-concurrency = <% .EndpointConcurrency %>
    <%- end %>

    # max count of tasks being handled, new tasks will be rejected.
    # end-point-max-tasks = 2000

    # set attributes about this server, e.g. { zone = "us-west-1", disk = "ssd" }.
    # labels = {}

    [storage]
    # set the path to rocksdb directory.
    # data-dir = "/tmp/tikv/store"

    # notify capacity of scheduler's channel
    # scheduler-notify-capacity = 10240

    # maximum number of messages can be processed in one tick
    # scheduler-messages-per-tick = 1024

    # the number of slots in scheduler latches, concurrency control for write.
    # scheduler-concurrency = 102400

    # scheduler's worker pool size, should increase it in heavy write cases,
    # also should less than total cpu cores.
    # scheduler-worker-pool-size = 4

    # When the number of concurrent running writes exceeds this threshold,
    # the "too busy" error is displayed.
    # scheduler-too-busy-threshold = 10000

    [pd]
    # pd endpoints
    # endpoints = []

    [metric]
    # the Prometheus client push interval. Setting the value to 0s stops Prometheus client from pushing.
    # interval = "15s"
    # the Prometheus pushgateway address. Leaving it empty stops Prometheus client from pushing.
    address = "<% .MetricsAddr %>"
    # the Prometheus client push job name. Note: A node id will automatically append, e.g., "tikv_1".
    # job = "tikv"

    [raftstore]
    # true (default value) for high reliability, this can prevent data loss when power failure.
    # sync-log = true

    # set the path to raftdb directory, default value is data-dir/raft
    # raftdb-path = ""

    # set store capacity, if no set, use disk capacity.
    # capacity = 0

    # notify capacity, 40960 is suitable for about 7000 regions.
    # notify-capacity = 40960

    # maximum number of messages can be processed in one tick.
    # messages-per-tick = 4096

    # Region heartbeat tick interval for reporting to pd.
    # pd-heartbeat-tick-interval = "60s"
    # Store heartbeat tick interval for reporting to pd.
    # pd-store-heartbeat-tick-interval = "10s"

    # When the region's size exceeds region-max-size, we will split the region
    # into two which the left region's size will be region-split-size or a little
    # bit smaller.
    # region-max-size = "144MB"
    # region-split-size = "96MB"
    # When region size changes exceeds region-split-check-diff, we should check
    # whether the region should be split or not.
    # region-split-check-diff = "6MB"

    # Interval to check region whether need to be split or not.
    # split-region-check-tick-interval = "10s"

    # When raft entry exceed the max size, reject to propose the entry.
    # raft-entry-max-size = "8MB"

    # Interval to gc unnecessary raft log.
    # raft-log-gc-tick-interval = "10s"
    # A threshold to gc stale raft log, must >= 1.
    # raft-log-gc-threshold = 50
    # When entry count exceed this value, gc will be forced trigger.
    # raft-log-gc-count-limit = 72000
    # When the approximate size of raft log entries exceed this value, gc will be forced trigger.
    # It's recommanded to set it to 3/4 of region-split-size.
    # raft-log-gc-size-limit = "72MB"

    # When a peer hasn't been active for max-peer-down-duration,
    # we will consider this peer to be down and report it to pd.
    # max-peer-down-duration = "5m"

    # Interval to check whether start manual compaction for a region,
    # 0 is the default value, means disable manual compaction.
    # region-compact-check-interval = "5m"
    # When delete keys of a region exceeds the size, a compaction will be started.
    # region-compact-delete-keys-count = 1000000
    # Interval to check whether should start a manual compaction for lock column family,
    # if written bytes reach lock-cf-compact-threshold for lock column family, will fire
    # a manual compaction for lock column family.
    # lock-cf-compact-interval = "10m"
    # lock-cf-compact-bytes-threshold = "256MB"

    # Interval (s) to check region whether the data are consistent.
    # consistency-check-interval = 0

    [rocksdb]
    # Maximum number of concurrent background jobs (compactions and flushes)
    # max-background-jobs = 8

    # This value represents the maximum number of threads that will concurrently perform a
    # compaction job by breaking it into multiple, smaller ones that are run simultaneously.
    # Default: 1 (i.e. no subcompactions)
    # max-sub-compactions = 1

    # Number of open files that can be used by the DB.  You may need to
    # increase this if your database has a large working set. Value -1 means
    # files opened are always kept open. You can estimate number of files based
    # on target_file_size_base and target_file_size_multiplier for level-based
    # compaction.
    # If max-open-files = -1, RocksDB will prefetch index and filter blocks into
    # block cache at startup, so if your database has a large working set, it will
    # take several minutes to open the db.
    # max-open-files = 40960

    # Max size of rocksdb's MANIFEST file.
    # For detailed explanation please refer to https://github.com/facebook/rocksdb/wiki/MANIFEST
    # max-manifest-file-size = "20MB"

    # If true, the database will be created if it is missing.
    # create-if-missing = true

    # rocksdb wal recovery mode
    # 0 : TolerateCorruptedTailRecords, tolerate incomplete record in trailing data on all logs;
    # 1 : AbsoluteConsistency, We don't expect to find any corruption in the WAL;
    # 2 : PointInTimeRecovery, Recover to point-in-time consistency;
    # 3 : SkipAnyCorruptedRecords, Recovery after a disaster;
    # wal-recovery-mode = 2

    # rocksdb write-ahead logs dir path
    # This specifies the absolute dir path for write-ahead logs (WAL).
    # If it is empty, the log files will be in the same dir as data.
    # When you set the path to rocksdb directory in memory like in /dev/shm, you may want to set
    # wal-dir to a directory on a persistent storage.
    # See https://github.com/facebook/rocksdb/wiki/How-to-persist-in-memory-RocksDB-database
    # wal-dir = "/tmp/tikv/store"

    # The following two fields affect how archived write-ahead logs will be deleted.
    # 1. If both set to 0, logs will be deleted asap and will not get into the archive.
    # 2. If wal-ttl-seconds is 0 and wal-size-limit is not 0,
    #    WAL files will be checked every 10 min and if total size is greater
    #    then wal-size-limit, they will be deleted starting with the
    #    earliest until size_limit is met. All empty files will be deleted.
    # 3. If wal-ttl-seconds is not 0 and wal-size-limit is 0, then
    #    WAL files will be checked every wal-ttl-seconds / 2 and those that
    #    are older than wal-ttl-seconds will be deleted.
    # 4. If both are not 0, WAL files will be checked every 10 min and both
    #    checks will be performed with ttl being first.
    # When you set the path to rocksdb directory in memory like in /dev/shm, you may want to set
    # wal-ttl-seconds to a value greater than 0 (like 86400) and backup your db on a regular basis.
    # See https://github.com/facebook/rocksdb/wiki/How-to-persist-in-memory-RocksDB-database
    # wal-ttl-seconds = 0
    # wal-size-limit = 0

    # rocksdb max total wal size
    # max-total-wal-size = "4GB"

    # rocksdb writable file max buffer size
    # writable-file-max-buffer-size = "1MB"

    # Rocksdb Statistics provides cumulative stats over time.
    # Turn statistics on will introduce about 5%-10% overhead for RocksDB,
    # but it is worthy to know the internal status of RocksDB.
    # enable-statistics = true

    # Dump statistics periodically in information logs.
    # Same as rocksdb's default value (10 min).
    # stats-dump-period = "10m"

    # Due to Rocksdb FAQ: https://github.com/facebook/rocksdb/wiki/RocksDB-FAQ,
    # If you want to use rocksdb on multi disks or spinning disks, you should set value at
    # least 2MB;
    # compaction-readahead-size = 0

    # This is the maximum buffer size that is used by WritableFileWrite
    # writable-file-max-buffer-size = "1MB"

    # Use O_DIRECT for both reads and writes in background flush and compactions
    # use-direct-io-for-flush-and-compaction = false

    # Limit the disk IO of compaction and flush. Compaction and flush can cause
    # terrible spikes if they exceed a certain threshold. Consider setting this to
    # 50% ~ 80% of the disk throughput for a more stable result. However, in heavy
    # write workload, limiting compaction and flush speed can cause write stalls too.
    # rate-bytes-per-sec = 0

    # Enable or disable the pipelined write
    # enable-pipelined-write = true

    # set backup path, if not set, use "backup" under store path.
    # backup-dir = "/tmp/tikv/store/backup"

    # Column Family default used to store actual data of the database.
    [rocksdb.defaultcf]
    # compression method (if any) is used to compress a block.
    #   no:     kNoCompression
    #   snappy: kSnappyCompression
    #   zlib:   kZlibCompression
    #   bzip2:  kBZip2Compression
    #   lz4:    kLZ4Compression
    #   lz4hc:  kLZ4HCCompression
    #   zstd:   kZSTD

    # per level compression
    # compression-per-level = ["no", "no", "lz4", "lz4", "lz4", "zstd", "zstd"]

    # Approximate size of user data packed per block.  Note that the
    # block size specified here corresponds to uncompressed data.
    # block-size = "64KB"

    # If you're doing point lookups you definitely want to turn bloom filters on, We use
    # bloom filters to avoid unnecessary disk reads. Default bits_per_key is 10, which
    # yields ~1% false positive rate. Larger bits_per_key values will reduce false positive
    # rate, but increase memory usage and space amplification.
    # bloom-filter-bits-per-key = 10

    # false means one sst file one bloom filter, true means evry block has a corresponding bloom filter
    # block-based-bloom-filter = false

    # level0-file-num-compaction-trigger = 4

    # Soft limit on number of level-0 files. We start slowing down writes at this point.
    # level0-slowdown-writes-trigger = 20

    # Maximum number of level-0 files.  We stop writes at this point.
    # level0-stop-writes-trigger = 36

    # Amount of data to build up in memory (backed by an unsorted log
    # on disk) before converting to a sorted on-disk file.
    # write-buffer-size = "128MB"

    # The maximum number of write buffers that are built up in memory.
    # max-write-buffer-number = 5

    # The minimum number of write buffers that will be merged together
    # before writing to storage.
    # min-write-buffer-number-to-merge = 1

    # Control maximum total data size for base level (level 1).
    # max-bytes-for-level-base = "512MB"

    # Target file size for compaction.
    # target-file-size-base = "8MB"

    # Max bytes for compaction.max_compaction_bytes
    # max-compaction-bytes = "2GB"

    # There are four different algorithms to pick files to compact.
    # 0 : ByCompensatedSize
    # 1 : OldestLargestSeqFirst
    # 2 : OldestSmallestSeqFirst
    # 3 : MinOverlappingRatio
    # compaction-pri = 3

    # block-cache used to cache uncompressed blocks, big block-cache can speed up read.
    # in normal cases should tune to 30%-50% system's total memory.
    <%- if eq .DefaultCFBlockCacheSize "" %>
    # block-cache-size = "1GB"
    <%- else %>
    block-cache-size = "<% .DefaultCFBlockCacheSize %>"
    <%- end %>

    # Indicating if we'd put index/filter blocks to the block cache.
    # If not specified, each "table reader" object will pre-load index/filter block
    # during table initialization.
    # cache-index-and-filter-blocks = true

    # Pin level0 filter and index blocks in cache.
    # pin-l0-filter-and-index-blocks = true

    # Enable read amplication statistics.
    # value  =>  memory usage (percentage of loaded blocks memory)
    # 1      =>  12.50 %
    # 2      =>  06.25 %
    # 4      =>  03.12 %
    # 8      =>  01.56 %
    # 16     =>  00.78 %
    # read-amp-bytes-per-bit = 0

    # Options for Column Family write
    # Column Family write used to store commit informations in MVCC model
    [rocksdb.writecf]
    # compression-per-level = ["no", "no", "lz4", "lz4", "lz4", "zstd", "zstd"]
    # block-size = "64KB"
    # write-buffer-size = "128MB"
    # max-write-buffer-number = 5
    # min-write-buffer-number-to-merge = 1
    # max-bytes-for-level-base = "512MB"
    # target-file-size-base = "8MB"

    # in normal cases should tune to 10%-30% system's total memory.
    <%- if eq .WriteCFBlockCacheSize "" %>
    # block-cache-size = "256MB"
    <%- else %>
    block-cache-size = "<% .WriteCFBlockCacheSize %>"
    <%- end %>
    # level0-file-num-compaction-trigger = 4
    # level0-slowdown-writes-trigger = 20
    # level0-stop-writes-trigger = 36
    # cache-index-and-filter-blocks = true
    # pin-l0-filter-and-index-blocks = true
    # compaction-pri = 3
    # read-amp-bytes-per-bit = 0

    [rocksdb.lockcf]
    # compression-per-level = ["no", "no", "no", "no", "no", "no", "no"]
    # block-size = "16KB"
    # write-buffer-size = "128MB"
    # max-write-buffer-number = 5
    # min-write-buffer-number-to-merge = 1
    # max-bytes-for-level-base = "128MB"
    # target-file-size-base = "8MB"
    # block-cache-size = "256MB"
    # level0-file-num-compaction-trigger = 1
    # level0-slowdown-writes-trigger = 20
    # level0-stop-writes-trigger = 36
    # cache-index-and-filter-blocks = true
    # pin-l0-filter-and-index-blocks = true
    # compaction-pri = 0
    # read-amp-bytes-per-bit = 0

    [raftdb]
    # max-sub-compactions = 1
    # max-open-files = 40960
    # max-manifest-file-size = "20MB"
    # create-if-missing = true
    # writable-file-max-buffer-size = "1MB"

    # enable-statistics = true
    # stats-dump-period = "10m"

    # compaction-readahead-size = 0
    # writable-file-max-buffer-size = "1MB"
    # use-direct-io-for-flush-and-compaction = false
    # enable-pipelined-write = true
    # allow-concurrent-memtable-write = false

    [raftdb.defaultcf]
    # compression-per-level = ["no", "no", "lz4", "lz4", "lz4", "zstd", "zstd"]
    # block-size = "64KB"
    # write-buffer-size = "128MB"
    # max-write-buffer-number = 5
    # min-write-buffer-number-to-merge = 1
    # max-bytes-for-level-base = "512MB"
    # target-file-size-base = "8MB"

    # should tune to 256MB~2GB.
    # block-cache-size = "256MB"
    # level0-file-num-compaction-trigger = 4
    # level0-slowdown-writes-trigger = 20
    # level0-stop-writes-trigger = 36
    # cache-index-and-filter-blocks = true
    # pin-l0-filter-and-index-blocks = true
    # compaction-pri = 0
    # read-amp-bytes-per-bit = 0

  prometheus-config: |-
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    <%- if eq .AlertmanagerURL "" %>
    <%- else %>
    alerting:
      alertmanagers:
      - static_configs:
        - targets:
          - <% .AlertmanagerURL %>
    <%- end %>
    scrape_configs:
      - job_name: 'tidb-cluster'
        scrape_interval: 15s
        honor_labels: true
        kubernetes_sd_configs:
        - role: pod
          namespaces:
            names:
            - <% .Namespace %>
        tls_config:
          insecure_skip_verify: true
        relabel_configs:
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_pod_node_name]
          action: replace
          target_label: kubernetes_node
        - source_labels: [__meta_kubernetes_pod_ip]
          action: replace
          target_label: kubernetes_pod_ip
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: instance
        - source_labels: [__meta_kubernetes_pod_label_cluster_pingcap_com_tidbCluster]
          action: replace
          target_label: cluster
    rule_files:
      - 'alert.rules'

  alert-rules-config: |-
    groups:
    - name: tidb-alert-rules
      rules:
      - alert: load_schema_fail
        expr: rate(tidb_domain_load_schema_total{type="failed"}[1m]) > 0
        for: 1s
        labels:
          env: '<% .ClusterName %>'
        annotations:
          description: 'alert: rate(tidb_domain_load_schema_total{type=''failed''} instance:
            <% .MetaInstance %> values: <% .MetaValue %>'
          summary: TiDB load schema fails
      - alert: local_shema_latency
        expr: histogram_quantile(1, rate(tidb_domain_load_schema_duration_bucket[5m])) > 5
        for: 1m
        labels:
          env: '<% .ClusterName %>'
        annotations:
          description: 'alert: histogram_quantile(1, rate(tidb_domain_load_schema_duration_bucket
            [5m])) instance: <% .MetaInstance %>  values: <% .MetaValue %>'
          summary: TiDB load schema latency is over 5s
      - alert: memery_abnormal
        expr: go_memstats_heap_inuse_bytes{job="tidb"} > 1e+09
        for: 10m
        labels:
          env: '<% .ClusterName %>'
        annotations:
          description: 'alert: go_memstats_heap_inuse_bytes{job=''tidb''} instance: <% .MetaInstance %>  values:
            <% .MetaValue %>'
          summary: TiDB mem heap is over 1GiB
      - alert: tidb_query_duration
        expr: histogram_quantile(0.99, sum(rate(tidb_server_handle_query_duration_seconds_bucket[1m]))
          BY (le, instance)) > 1
        for: 5s
        labels:
          env: '<% .ClusterName %>'
        annotations:
          description: 'instance: <% .MetaInstance %> values: <% .MetaValue %> alert: histogram_quantile(0.99,
            sum(rate(tidb_server_handle_query_duration_seconds_bucket[1m])) by (le, instance))
            > 1 .'
          summary: TiDB query duration 99th percentile is above 1s
      - alert: tidb_tikvclient_region_err
        expr: sum(rate(tidb_tikvclient_region_err_total{type="server_is_busy"}[1m])) > 0
        for: 1m
        labels:
          channels: alerts
          env: '<% .ClusterName %>'
        annotations:
          description: 'alert: sum(rate(tidb_tikvclient_region_err_total{type=''server_is_busy''}[1m]))
            instance: <% .MetaInstance %> values: <% .MetaValue %>'
          summary: TiDB server is busy
      - alert: tikv_raft_process_ready
        expr: sum(rate(tikv_raftstore_raft_process_nanos_total{type="ready"}[1m])) BY
          (type, instance) / 1e+09 > 1
        for: 1m
        labels:
          channels: alerts
          env: '<% .ClusterName %>'
        annotations:
          description: 'alert: sum(rate(tikv_raftstore_raft_process_nanos_total{type=''ready''[1m]))
            by (type, instance) / 1000000000 instance: <% .MetaInstance %> values: <% .MetaValue %>'
          summary: TiKV raft process ready duration is above 1s
      - alert: raft_sotre_msg
        expr: sum(rate(tikv_server_report_failure_msg_total{type="unreachable"}[1m])) > 10
        for: 1m
        labels:
          channels: alerts
          env: '<% .ClusterName %>'
        annotations:
          description: 'alert: sum(rate(tikv_server_raft_store_msg_total{type=''unreachable''}[1m]))
            > 10  values:<% .MetaValue %>'
          summary: TiKV too many unreachable raft stores
      - alert: tikv_channel_full_total
        expr: sum(rate(tikv_channel_full_total[1m])) BY (type, instance) > 0
        for: 1s
        labels:
          channels: alerts
          env: '<% .ClusterName %>'
        annotations:
          description: 'alert: sum(rate(tikv_channel_full_total[1m])) by (type, instance)  instance:
            <% .MetaInstance %>  values: <% .MetaValue %>'
          summary: TiKV channel full
      - alert: coprocessor_pending_request
        expr: sum(rate(tikv_coprocessor_pending_request[1m])) BY (type, instance) > 2
        for: 10s
        labels:
          env: '<% .ClusterName %>'
        annotations:
          description: 'alert: sum(rate(tikv_coprocessor_pending_request[1m])) by (type,instance)
            > 2 type: {{.MetaType}} instance: <% .MetaInstance %>  values: <% .MetaValue %>'
          summary: TiKV pending {{.MetaType}} request is high
      - alert: tikv_scheduler_context_total
        expr: sum(tikv_scheduler_contex_total) BY (job) > 300
        for: 2m
        labels:
          env: '<% .ClusterName %>'
        annotations:
          description: 'alert: sum(tikv_scheduler_contex_total) by (job) > 300 instance:
            <% .MetaInstance %>  values: <% .MetaValue %>'
          summary: TiKV scheduler context total
      - alert: tikv_thread_cpu_seconds_total
        expr: rate(tikv_thread_cpu_seconds_total{name="raftstore"}[1m]) > 0.8
        for: 1m
        labels:
          env: '<% .ClusterName %>'
        annotations:
          description: 'alert: rate(tikv_thread_cpu_seconds_total{name=''raftstore''}[1m])
            > 0.8 instance <% .MetaInstance %> values: <% .MetaValue %>'
          summary: TiKV raftstore thread CPU seconds is high
      - alert: tikv_thread_cpu_seconds_total
        expr: rate(tikv_thread_cpu_seconds_total{name="endpoint-pool"}[1m]) > 0.9
        for: 1m
        labels:
          env: '<% .ClusterName %>'
        annotations:
          description: 'alert: rate(tikv_thread_cpu_seconds_total{name=''endpoint-pool''}[1m])
            > 0.9 instance <% .MetaInstance %> values: <% .MetaValue %>'
          summary: TiKV endpoint-pool thread CPU seconds is high
      - alert: tikv_thread_cpu_seconds_total
        expr: rate(tikv_thread_cpu_seconds_total{name="sched-worker-pool"}[1m]) > 0.9
        for: 1m
        labels:
          env: '<% .ClusterName %>'
        annotations:
          description: 'alert: rate(tikv_thread_cpu_seconds_total{name=''sched-worker-pool''}[1m])
            > 0.9 instance <% .MetaInstance %> values: <% .MetaValue %>'
          summary: TiKV sched-worker-pool thread CPU seconds is high
      - alert: tikv_leader_drops
        expr: delta(tikv_pd_heartbeat_tick_total{type="leader"}[30s]) < -10
        for: 1s
        labels:
          channels: alerts
          env: '<% .ClusterName %>'
        annotations:
          description: 'alert: delta(tikv_pd_heartbeat_tick_total{type=''leader''}[30s])
            > 10 instance: <% .MetaInstance %>   values:<% .MetaValue %>'
          summary: TiKV leader drops
      - alert: etcd_disk_fsync
        expr: sum(rate(etcd_disk_wal_fsync_duration_seconds_count[1m])) BY (instance) == 0
        for: 1m
        labels:
          channels: alerts
          env: '<% .ClusterName %>'
        annotations:
          description: 'alert: sum(rate(etcd_disk_wal_fsync_duration_seconds_count[1m]))
            by (instance) instance: <% .MetaInstance %>   values:<% .MetaValue %>'
          summary: PD etcd disk fsync is down
